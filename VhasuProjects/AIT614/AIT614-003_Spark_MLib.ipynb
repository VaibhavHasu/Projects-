{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d4bf5c-0b0d-4ab3-8013-0702a66a5da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### AIT 614 - Big Data Essentials <br>\n",
    "#### Project Title: FHWA Bridge Conditions Analysis Using Big Data Techniques\n",
    "#### 2. Spark MLib\n",
    "#### TEAM 4\n",
    "<hr>\n",
    "\n",
    "Course Section #: AIT 614 - 003 <br>\n",
    "#### Team Members\n",
    "1. Aryan Patel Kolagani - G01517560 <br>\n",
    "2. Rithvik Madhavaram - G01501806 <br>\n",
    "3. Chetan Muppavarapu - G01504057 <br>\n",
    "4. Srivaths Nrusimha Rao Chengal - G01512113 <br>\n",
    "5. Vaibhav Hasu - G01517039 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db4398f-bd81-4fb7-895c-184bb7ea943a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Decision Tree RMSE: 1.7279883119350314\nðŸŒ² Decision Tree RÂ²: -0.06034006342289566\nðŸŒ²ðŸŒ² Random Forest RMSE: 1.6827032018629609\nðŸŒ²ðŸŒ² Random Forest RÂ²: -0.005491982326457423\nðŸŽ¯ Random Forest Feature Importances:\n(8,[0,1,2,3,4,5,6,7],[0.12403230123124924,0.13536675037554405,0.12376594697290043,0.11523633305261992,0.16931762920872434,0.12251391006732983,0.1285627380364993,0.08120439105513279])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Loading dataset\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/akolagan@gmu.edu/FHWA_Bridge_Conditions_Dataset.csv\")\n",
    "\n",
    "# Cleaning all numeric columns\n",
    "columns_to_clean = [\n",
    "    \"Age_Years\", \"Daily_Traffic\", \"Repair_Cost_USD\", \"Truck_Percentage\", \n",
    "    \"Deterioration_Rate\", \"Deck_Condition\", \"Length_Meters\", \"Repair_Time_Days\"\n",
    "]\n",
    "\n",
    "for col_name in columns_to_clean:\n",
    "    df = df.withColumn(f\"{col_name}_clean\", regexp_replace(col(col_name), \"[$,%]\", \"\"))\n",
    "    df = df.withColumn(f\"{col_name}_clean\", col(f\"{col_name}_clean\").cast(\"double\"))\n",
    "\n",
    "# Droping rows with nulls in key fields\n",
    "df_clean = df.dropna(subset=[f\"{c}_clean\" for c in columns_to_clean] + [\"Material\"])\n",
    "\n",
    "# Renaming cleaned columns\n",
    "df_clean = df_clean \\\n",
    "    .drop(*columns_to_clean) \\\n",
    "    .withColumnRenamed(\"Age_Years_clean\", \"Age_Years\") \\\n",
    "    .withColumnRenamed(\"Daily_Traffic_clean\", \"Daily_Traffic\") \\\n",
    "    .withColumnRenamed(\"Repair_Cost_USD_clean\", \"Repair_Cost_USD\") \\\n",
    "    .withColumnRenamed(\"Truck_Percentage_clean\", \"Truck_Percentage\") \\\n",
    "    .withColumnRenamed(\"Deterioration_Rate_clean\", \"Deterioration_Rate\") \\\n",
    "    .withColumnRenamed(\"Deck_Condition_clean\", \"Deck_Condition\") \\\n",
    "    .withColumnRenamed(\"Length_Meters_clean\", \"Length_Meters\") \\\n",
    "    .withColumnRenamed(\"Repair_Time_Days_clean\", \"Repair_Time_Days\")\n",
    "\n",
    "# Encoding the 'Material' column\n",
    "indexer = StringIndexer(inputCol=\"Material\", outputCol=\"Material_Indexed\")\n",
    "df_encoded = indexer.fit(df_clean).transform(df_clean)\n",
    "\n",
    "# Assembling feature vector\n",
    "feature_cols = [\n",
    "    \"Age_Years\", \"Daily_Traffic\", \"Repair_Cost_USD\", \"Truck_Percentage\", \n",
    "    \"Deterioration_Rate\", \"Length_Meters\", \"Repair_Time_Days\", \"Material_Indexed\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_features = assembler.transform(df_encoded).select(\"Bridge_ID\", \"Deck_Condition\", \"features\")\n",
    "\n",
    "# KMeans Clustering\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"features\")\n",
    "kmeans_model = kmeans.fit(df_features)\n",
    "df_clustered = kmeans_model.transform(df_features)\n",
    "\n",
    "# Saving cluster output \n",
    "df_clustered.select(\"Bridge_ID\", \"Deck_Condition\", \"prediction\") \\\n",
    "    .write.mode(\"overwrite\").csv(\"/dbfs/FileStore/bridge_cluster_output.csv\", header=True)\n",
    "\n",
    "# Decision tree regression\n",
    "df_regression = df_clustered.drop(\"prediction\")\n",
    "\n",
    "train_dt, test_dt = df_regression.randomSplit([0.8, 0.2], seed=123)\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Deck_Condition\")\n",
    "dt_model = dt.fit(train_dt)\n",
    "dt_predictions = dt_model.transform(test_dt)\n",
    "\n",
    "# Evaluating Decision Tree\n",
    "evaluator = RegressionEvaluator(labelCol=\"Deck_Condition\", predictionCol=\"prediction\")\n",
    "rmse_dt = evaluator.setMetricName(\"rmse\").evaluate(dt_predictions)\n",
    "r2_dt = evaluator.setMetricName(\"r2\").evaluate(dt_predictions)\n",
    "\n",
    "print(f\"ðŸŒ² Decision Tree RMSE: {rmse_dt}\")\n",
    "print(f\"ðŸŒ² Decision Tree RÂ²: {r2_dt}\")\n",
    "\n",
    "# Saving Decision Tree predictions\n",
    "dt_predictions.select(\"Deck_Condition\", \"prediction\") \\\n",
    "    .write.mode(\"overwrite\").csv(\"/dbfs/FileStore/deck_condition_predictions_dt.csv\", header=True)\n",
    "\n",
    "# Random Forest Regression\n",
    "train_rf, test_rf = df_regression.randomSplit([0.8, 0.2], seed=123)\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Deck_Condition\", numTrees=50)\n",
    "rf_model = rf.fit(train_rf)\n",
    "rf_predictions = rf_model.transform(test_rf)\n",
    "\n",
    "# Evaluating Random Forest\n",
    "rmse_rf = evaluator.setMetricName(\"rmse\").evaluate(rf_predictions)\n",
    "r2_rf = evaluator.setMetricName(\"r2\").evaluate(rf_predictions)\n",
    "\n",
    "print(f\"ðŸŒ²ðŸŒ² Random Forest RMSE: {rmse_rf}\")\n",
    "print(f\"ðŸŒ²ðŸŒ² Random Forest RÂ²: {r2_rf}\")\n",
    "\n",
    "# Saving Random Forest predictions\n",
    "rf_predictions.select(\"Deck_Condition\", \"prediction\") \\\n",
    "    .write.mode(\"overwrite\").csv(\"/dbfs/FileStore/deck_condition_predictions_rf.csv\", header=True)\n",
    "\n",
    "# Feature importances\n",
    "print(\"ðŸŽ¯ Random Forest Feature Importances:\")\n",
    "print(rf_model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e341e43-3af0-44b4-9992-5272e0a7c3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Clustered Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a167d2-68b6-45a9-aabd-08469166e434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered DataFrame:\n+---------+--------------+--------------------+----------+\n|Bridge_ID|Deck_Condition|            features|prediction|\n+---------+--------------+--------------------+----------+\n|   100000|           6.0|[17.0,12899.0,431...|         0|\n|   100001|           6.0|[52.0,47458.0,188...|         1|\n|   100002|           3.0|[35.0,68505.0,364...|         2|\n|   100003|           4.0|[60.0,10563.0,182...|         1|\n|   100004|           8.0|[90.0,157293.0,18...|         1|\n+---------+--------------+--------------------+----------+\nonly showing top 5 rows\n\nRandom Forest Predictions:\n+--------------+------------------+\n|Deck_Condition|        prediction|\n+--------------+------------------+\n|           3.0| 5.497897332790483|\n|           8.0| 5.365861276270184|\n|           7.0| 5.386994707482964|\n|           4.0| 6.255114718875766|\n|           3.0|5.5322452783240115|\n+--------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Clustered DataFrame:\")\n",
    "df_clustered.show(5)\n",
    "print(\"Random Forest Predictions:\")\n",
    "rf_predictions.select(\"Deck_Condition\", \"prediction\").show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AIT614-003_Spark_MLib",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}